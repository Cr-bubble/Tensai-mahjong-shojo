{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discard Model: NN_v1\n",
    "\n",
    "We use tensorflow to train our NN model.\n",
    "\n",
    "Testdata from [Japanese Mahjong Board States](https://www.kaggle.com/datasets/trongdt/japanese-mahjong-board-states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(34,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(34, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 載入數據\n",
    "# folder_path = 'dataset_extracted_2'\n",
    "# all_data = pd.DataFrame()\n",
    "# dataframes = []\n",
    "# cnt = 0\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.csv'):\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         data = pd.read_csv(file_path)\n",
    "#         dataframes.append(data)\n",
    "#         cnt+=1\n",
    "#         if(cnt > 2000):\n",
    "#             break\n",
    "\n",
    "# all_data = pd.concat(dataframes, ignore_index=True)\n",
    "# X = all_data.iloc[:, 68:102].values\n",
    "# Y = all_data.iloc[:, -1].values\n",
    "# Y = tf.keras.utils.to_categorical(Y)\n",
    "# np.save('dataX.npy', X)\n",
    "# np.save('dataY.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776752, 34)\n",
      "(776752, 34)\n",
      "(194188, 34)\n",
      "(194188, 34)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('dataX.npy')\n",
    "Y = np.load('dataY.npy')\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='checkpoints/model_epoch_{epoch:02d}.h5', \n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12137/12137 [==============================] - 76s 6ms/step - loss: 2.0173 - accuracy: 0.3691 - val_loss: 1.7212 - val_accuracy: 0.4294\n",
      "\n",
      "Epoch 00001: saving model to checkpoints\\model_epoch_01.h5\n",
      "Epoch 2/10\n",
      "12137/12137 [==============================] - 80s 7ms/step - loss: 1.7212 - accuracy: 0.4337 - val_loss: 1.6232 - val_accuracy: 0.4578\n",
      "\n",
      "Epoch 00002: saving model to checkpoints\\model_epoch_02.h5\n",
      "Epoch 3/10\n",
      "12137/12137 [==============================] - 79s 6ms/step - loss: 1.6486 - accuracy: 0.4527 - val_loss: 1.5709 - val_accuracy: 0.4703\n",
      "\n",
      "Epoch 00003: saving model to checkpoints\\model_epoch_03.h5\n",
      "Epoch 4/10\n",
      "12137/12137 [==============================] - 82s 7ms/step - loss: 1.6126 - accuracy: 0.4626 - val_loss: 1.5521 - val_accuracy: 0.4794\n",
      "\n",
      "Epoch 00004: saving model to checkpoints\\model_epoch_04.h5\n",
      "Epoch 5/10\n",
      "12137/12137 [==============================] - 78s 6ms/step - loss: 1.5875 - accuracy: 0.4688 - val_loss: 1.5261 - val_accuracy: 0.4834\n",
      "\n",
      "Epoch 00005: saving model to checkpoints\\model_epoch_05.h5\n",
      "Epoch 6/10\n",
      "12137/12137 [==============================] - 76s 6ms/step - loss: 1.5677 - accuracy: 0.4751 - val_loss: 1.5001 - val_accuracy: 0.4964\n",
      "\n",
      "Epoch 00006: saving model to checkpoints\\model_epoch_06.h5\n",
      "Epoch 7/10\n",
      "12137/12137 [==============================] - 77s 6ms/step - loss: 1.5514 - accuracy: 0.4795 - val_loss: 1.4804 - val_accuracy: 0.4993\n",
      "\n",
      "Epoch 00007: saving model to checkpoints\\model_epoch_07.h5\n",
      "Epoch 8/10\n",
      "12137/12137 [==============================] - 76s 6ms/step - loss: 1.5371 - accuracy: 0.4839 - val_loss: 1.4897 - val_accuracy: 0.4916\n",
      "\n",
      "Epoch 00008: saving model to checkpoints\\model_epoch_08.h5\n",
      "Epoch 9/10\n",
      "12137/12137 [==============================] - 79s 6ms/step - loss: 1.5264 - accuracy: 0.4866 - val_loss: 1.4727 - val_accuracy: 0.4983\n",
      "\n",
      "Epoch 00009: saving model to checkpoints\\model_epoch_09.h5\n",
      "Epoch 10/10\n",
      "12137/12137 [==============================] - 77s 6ms/step - loss: 1.5184 - accuracy: 0.4886 - val_loss: 1.4527 - val_accuracy: 0.5094\n",
      "\n",
      "Epoch 00010: saving model to checkpoints\\model_epoch_10.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29321c815b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints\\model_epoch_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prediction(input_tiles, prediction):\n",
    "    legal_tiles = np.where(input_tiles > 0)[1]\n",
    "    prediction = prediction.reshape(34,)\n",
    "    filtered_prediction = np.zeros_like(prediction)\n",
    "    filtered_prediction[legal_tiles] = prediction[legal_tiles]\n",
    "    return np.argmax(filtered_prediction)\n",
    "\n",
    "def discard(hands_input):\n",
    "    hands = np.zeros(34) \n",
    "\n",
    "    for tile in hands_input:\n",
    "        hands[tile['type'] * 9 + tile['index'] - 1] += 1\n",
    "\n",
    "    hands = hands.reshape(1, 34)\n",
    "    out = filter_prediction(hands, model.predict(hands))\n",
    "\n",
    "    for i, tile in enumerate(hands_input):\n",
    "        if tile['type'] * 9 + tile['index'] - 1 == out:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000292F6654670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hands = [1, 0, 0, 0, 0, 0, 0, 0, 0,  3, 0, 0, 0, 0, 0, 0, 0, 1,  1, 0, 0, 0, 0, 0, 0, 0, 1,  1, 1, 1, 1, 1, 1, 1]\n",
    "#new =   [0, 0, 0, 0, 0, 0, 0, 0, 0,  1, 0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 0, 0, 0, 0, 0]\n",
    "new = 9\n",
    "\n",
    "hands_input = [\n",
    "    {'dora': False, 'type': 0, 'index': 3},\n",
    "    {'dora': False, 'type': 0, 'index': 2},\n",
    "    {'dora': False, 'type': 0, 'index': 3},\n",
    "    {'dora': False, 'type': 1, 'index': 4},\n",
    "    {'dora': False, 'type': 1, 'index': 5},\n",
    "    {'dora': False, 'type': 1, 'index': 6},\n",
    "    {'dora': False, 'type': 1, 'index': 7},\n",
    "    {'dora': False, 'type': 1, 'index': 8},\n",
    "    {'dora': False, 'type': 1, 'index': 9},\n",
    "    {'dora': False, 'type': 2, 'index': 0},\n",
    "    {'dora': False, 'type': 3, 'index': 1},\n",
    "    {'dora': False, 'type': 3, 'index': 2},\n",
    "    {'dora': False, 'type': 3, 'index': 3},\n",
    "    {'dora': False, 'type': 3, 'index': 3}\n",
    "]\n",
    "print(discard(hands_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize into Class\n",
    "merge above code into a single class, for the convenience of future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_NN():\n",
    "    def __init__(self) -> None:\n",
    "        self.model = Sequential([Dense(64, activation='relu', input_shape=(34,)),\n",
    "                                 Dense(128, activation='relu'),\n",
    "                                 Dense(256, activation='relu'),\n",
    "                                 Dropout(0.2),\n",
    "                                 Dense(34, activation='softmax')\n",
    "                                ])\n",
    "        self.model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3),loss='categorical_crossentropy', metrics=['accuracy'])       \n",
    "        self.model.load_weights('checkpoints\\model_epoch_10.h5')\n",
    "\n",
    "    def filter_prediction(self, input_tiles, prediction):\n",
    "        \"\"\"\n",
    "        filter the prediction to make sure the output is a legal tile index\n",
    "        \n",
    "        Args:\n",
    "            input_tiles (np.array): player's hand (length = 34 vec, one-hot encoding)\n",
    "            prediction (np.array): model's prediction (length = 34 vec, softmax output)\n",
    "        Returns:\n",
    "            int: filtered prediction index\n",
    "        \"\"\"\n",
    "        legal_tiles = np.where(input_tiles > 0)[1] \n",
    "        prediction = prediction.reshape(34,)\n",
    "        filtered_prediction = np.zeros_like(prediction)\n",
    "        filtered_prediction[legal_tiles] = prediction[legal_tiles]\n",
    "        \n",
    "        return np.argmax(filtered_prediction)\n",
    "\n",
    "    def discard(self, hands_input):\n",
    "        hands = np.zeros(34)\n",
    "\n",
    "        for tile in hands_input:\n",
    "            hands[tile['type'] * 9 + tile['index'] - 1] += 1\n",
    "\n",
    "        hands = hands.reshape(1, 34)\n",
    "        out = self.filter_prediction(hands, self.model.predict(hands))\n",
    "        \n",
    "        for i, tile in enumerate(hands_input):\n",
    "            if tile['type'] * 9 + tile['index'] - 1 == out:\n",
    "                return i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
